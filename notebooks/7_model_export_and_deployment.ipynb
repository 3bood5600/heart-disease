{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fce7ee11",
   "metadata": {},
   "source": [
    "# 2.7 Model Export & Deployment\n",
    "\n",
    "This notebook finalizes the Heart Disease UCI model by rebuilding a full preprocessing + model inference pipeline and exporting it for deployment. It reproduces evaluation metrics (Accuracy ≈ 0.88, F1 ≈ 0.89, ROC AUC ≈ 0.905) for the **threshold-tuned SVC** selected in the previous hyperparameter tuning stage.\n",
    "\n",
    "**Key Outputs**\n",
    "- Deployed pipeline (`../models/best_model.pkl`)\n",
    "- Model report (`../results/best_model_report.json`)\n",
    "- Verification summary (final cell)\n",
    "\n",
    "> All steps are deterministic with `random_state=42` for reproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d81665e",
   "metadata": {},
   "source": [
    "## 1. Imports & Reproducibility Setup\n",
    "Load core libraries and ensure directories for models/results exist. We re-instantiate the chosen SVC with the tuned hyperparameters and will later apply the selected decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2c033838",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configured Threshold-Tuned SVC best_params: {'C': 1.5, 'kernel': 'rbf', 'gamma': 'scale', 'probability': True, 'class_weight': None, 'random_state': 42}\n",
      "Selected decision threshold: 0.617\n"
     ]
    }
   ],
   "source": [
    "# Imports & reproducibility setup\n",
    "import os, json, joblib, warnings, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, classification_report\n",
    "warnings.filterwarnings('ignore')\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "# Paths\n",
    "ROOT = Path('..')\n",
    "DATA_PATH = ROOT / 'data' / 'selected_features.csv'\n",
    "MODELS_DIR = ROOT / 'models'\n",
    "RESULTS_DIR = ROOT / 'results'\n",
    "MODELS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "RESULTS_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "# Attempt to hydrate BEST_PARAMS & threshold from prior tuning artifacts if present\n",
    "REPORT_JSON_PATH = RESULTS_DIR / 'best_model_report.json'\n",
    "DEFAULT_BEST_PARAMS = {\n",
    "    'C': 1.5,          # fallback tuned C (will be overridden if report exists)\n",
    "    'kernel': 'rbf',\n",
    "    'gamma': 'scale',  # fallback\n",
    "    'probability': True,\n",
    "    'class_weight': None,\n",
    "    'random_state': RANDOM_STATE\n",
    "}\n",
    "DEFAULT_THRESHOLD = 0.42\n",
    "\n",
    "if REPORT_JSON_PATH.exists():\n",
    "    try:\n",
    "        with open(REPORT_JSON_PATH, 'r', encoding='utf-8') as f:\n",
    "            _rep = json.load(f)\n",
    "        if _rep.get('model_name') == 'Threshold-Tuned SVC':\n",
    "            BEST_PARAMS = _rep.get('best_params', DEFAULT_BEST_PARAMS)\n",
    "            SELECTED_THRESHOLD = _rep.get('best_threshold', DEFAULT_THRESHOLD)\n",
    "        else:\n",
    "            BEST_PARAMS = DEFAULT_BEST_PARAMS\n",
    "            SELECTED_THRESHOLD = DEFAULT_THRESHOLD\n",
    "    except Exception as e:\n",
    "        print('[WARN] Failed to read existing report, using defaults:', e)\n",
    "        BEST_PARAMS = DEFAULT_BEST_PARAMS\n",
    "        SELECTED_THRESHOLD = DEFAULT_THRESHOLD\n",
    "else:\n",
    "    BEST_PARAMS = DEFAULT_BEST_PARAMS\n",
    "    SELECTED_THRESHOLD = DEFAULT_THRESHOLD\n",
    "\n",
    "print('Configured Threshold-Tuned SVC best_params:', BEST_PARAMS)\n",
    "print('Selected decision threshold:', SELECTED_THRESHOLD)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b885f9",
   "metadata": {},
   "source": [
    "## 2. Load Cleaned Dataset\n",
    "Load the engineered, feature-selected dataset `selected_features.csv` from `../data/`. The target column is inferred (`target` or last column)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad8cb092",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded dataset: ..\\data\\selected_features.csv shape= (920, 18)\n",
      "Feature columns: 17\n",
      "Feature columns: 17\n",
      "Target distribution: {1: 0.553, 0: 0.447}\n",
      "Target distribution: {1: 0.553, 0: 0.447}\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f'Missing dataset: {DATA_PATH}. Ensure preprocessing notebook exported selected_features.csv')\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print('Loaded dataset:', DATA_PATH, 'shape=', df.shape)\n",
    "TARGET_COL = 'target' if 'target' in df.columns else df.columns[-1]\n",
    "X = df.drop(columns=[TARGET_COL]).copy()\n",
    "y = df[TARGET_COL].copy()\n",
    "print('Feature columns:', len(X.columns))\n",
    "print('Target distribution:', y.value_counts(normalize=True).round(3).to_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f504ec6",
   "metadata": {},
   "source": [
    "## 3. Train/Test Split\n",
    "Hold out 20% of the data for final evaluation. Stratified split preserves class balance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "94ef4e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (736, 17) Test: (184, 17)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "strat = y if y.nunique() > 1 else None\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=strat\n",
    ")\n",
    "print('Train:', X_train.shape, 'Test:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046d3c2f",
   "metadata": {},
   "source": [
    "## 4. Rebuild Preprocessing + Threshold-Tuned SVC Pipeline\n",
    "We:\n",
    "1. Identify numeric vs categorical columns.\n",
    "2. Apply scaling to numeric features and one-hot encode categoricals.\n",
    "3. Fit the SVC with tuned hyperparameters on training data.\n",
    "4. Apply a custom prediction helper that enforces the selected decision threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ddc4835c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric cols: 8 | Categorical cols: 9\n",
      "Pipeline fitted.\n",
      "Pipeline fitted.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "# Split feature types\n",
    "numeric_cols = X_train.select_dtypes(include=['int64','int32','float64','float32']).columns.tolist()\n",
    "categorical_cols = [c for c in X_train.columns if c not in numeric_cols]\n",
    "print(f'Numeric cols: {len(numeric_cols)} | Categorical cols: {len(categorical_cols)}')\n",
    "\n",
    "numeric_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "cat_pipeline = Pipeline([\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_pipeline, numeric_cols),\n",
    "    ('cat', cat_pipeline, categorical_cols)\n",
    "])\n",
    "\n",
    "# Threshold wrapper\n",
    "class ThresholdSVC(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, threshold=0.5, **svc_params):\n",
    "        self.threshold = threshold\n",
    "        self.svc_params = svc_params\n",
    "        self.model_ = SVC(**svc_params)\n",
    "    def fit(self, X, y):\n",
    "        self.model_.fit(X, y)\n",
    "        return self\n",
    "    def predict(self, X):\n",
    "        if hasattr(self.model_, 'predict_proba'):\n",
    "            prob = self.model_.predict_proba(X)[:,1]\n",
    "        else:\n",
    "            scores = self.model_.decision_function(X)\n",
    "            mn, mx = scores.min(), scores.max()\n",
    "            prob = (scores - mn)/(mx - mn + 1e-9)\n",
    "        return (prob >= self.threshold).astype(int)\n",
    "    def predict_proba(self, X):\n",
    "        if hasattr(self.model_, 'predict_proba'):\n",
    "            return self.model_.predict_proba(X)\n",
    "        scores = self.model_.decision_function(X)\n",
    "        mn, mx = scores.min(), scores.max()\n",
    "        prob_pos = (scores - mn)/(mx - mn + 1e-9)\n",
    "        prob = np.vstack([1 - prob_pos, prob_pos]).T\n",
    "        return prob\n",
    "    def get_params(self, deep=True):\n",
    "        return {'threshold': self.threshold, **self.svc_params}\n",
    "    def set_params(self, **params):\n",
    "        if 'threshold' in params:\n",
    "            self.threshold = params.pop('threshold')\n",
    "        self.svc_params.update(params)\n",
    "        self.model_.set_params(**self.svc_params)\n",
    "        return self\n",
    "\n",
    "svc_core = ThresholdSVC(threshold=SELECTED_THRESHOLD, **BEST_PARAMS)\n",
    "\n",
    "final_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('svc', svc_core)\n",
    "])\n",
    "\n",
    "final_pipeline.fit(X_train, y_train)\n",
    "print('Pipeline fitted.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953e666c",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "Compute performance metrics on the held-out test split and verify they align with prior tuning (Accuracy ≈ 0.88, F1 ≈ 0.89, ROC AUC ≈ 0.905)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "43cf7304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed metrics (threshold applied): {'accuracy': 0.875, 'precision': 0.8911, 'recall': 0.8824, 'f1': 0.8867, 'roc_auc': 0.9063}\n",
      "\n",
      "Reference check:\n",
      " accuracy: got 0.8750 target 0.8804 Δ=-0.0054 [OK]\n",
      " f1: got 0.8867 target 0.8932 Δ=-0.0065 [OK]\n",
      " roc_auc: got 0.9063 target 0.9052 Δ=+0.0011 [OK]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "# Predictions using thresholded pipeline\n",
    "y_pred = final_pipeline.predict(X_test)\n",
    "# For AUC need probabilities (post-preprocessor). Using predict_proba wrapper\n",
    "proba = final_pipeline.predict_proba(X_test)\n",
    "prob_pos = proba[:,1]\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "recall = recall_score(y_test, y_pred, zero_division=0)\n",
    "f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "roc_auc = roc_auc_score(y_test, prob_pos)\n",
    "\n",
    "metrics = {\n",
    "    'accuracy': round(float(accuracy), 4),\n",
    "    'precision': round(float(precision), 4),\n",
    "    'recall': round(float(recall), 4),\n",
    "    'f1': round(float(f1), 4),\n",
    "    'roc_auc': round(float(roc_auc), 4)\n",
    "}\n",
    "print('Computed metrics (threshold applied):', metrics)\n",
    "\n",
    "# Target (reference) metrics for sanity (tolerance allowed due to randomness / splits)\n",
    "TARGET_METRICS = {'accuracy': 0.8804, 'f1': 0.8932, 'roc_auc': 0.9052}\n",
    "TOL = 0.015\n",
    "print('\\nReference check:')\n",
    "for k, v in TARGET_METRICS.items():\n",
    "    diff = metrics[k] - v\n",
    "    status = 'OK' if abs(diff) <= TOL else 'DRIFT'\n",
    "    print(f\" {k}: got {metrics[k]:.4f} target {v:.4f} Δ={diff:+.4f} [{status}]\")\n",
    "\n",
    "metrics['status'] = {k: ('OK' if abs(metrics[k]-TARGET_METRICS[k])<=TOL else 'DRIFT') for k in TARGET_METRICS}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994840fc",
   "metadata": {},
   "source": [
    "## 6. Save Exported Pipeline & Report\n",
    "Persist the full preprocessing + SVC pipeline and a JSON report containing model metadata, threshold, metrics, and reproducibility info."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "d968f967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved pipeline -> ..\\models\\best_model.pkl\n",
      "Saved report -> ..\\results\\best_model_report.json\n",
      "\n",
      "Report snapshot:\n",
      " accuracy: 0.875\n",
      " precision: 0.8911\n",
      " recall: 0.8824\n",
      " f1: 0.8867\n",
      " roc_auc: 0.9063\n",
      "\n",
      "Report snapshot:\n",
      " accuracy: 0.875\n",
      " precision: 0.8911\n",
      " recall: 0.8824\n",
      " f1: 0.8867\n",
      " roc_auc: 0.9063\n"
     ]
    }
   ],
   "source": [
    "# Save pipeline and report (force sync with threshold 0.617 & current metrics)\n",
    "MODEL_EXPORT_PATH = MODELS_DIR / 'best_model.pkl'\n",
    "REPORT_PATH = RESULTS_DIR / 'best_model_report.json'\n",
    "\n",
    "joblib.dump(final_pipeline, MODEL_EXPORT_PATH)\n",
    "print('Saved pipeline ->', MODEL_EXPORT_PATH)\n",
    "\n",
    "report = {\n",
    "    'model_name': 'Threshold-Tuned SVC',\n",
    "    'best_params': BEST_PARAMS,\n",
    "    'best_threshold': SELECTED_THRESHOLD,\n",
    "    'metrics': {k: metrics[k] for k in ['accuracy','precision','recall','f1','roc_auc']},\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'cv_strategy': '5-Fold StratifiedKFold',\n",
    "    'preprocessing': {\n",
    "        'numeric_scaling': 'StandardScaler',\n",
    "        'categorical_encoding': 'OneHotEncoder(ignore_unknown)',\n",
    "        'numeric_features': numeric_cols,\n",
    "        'categorical_features': categorical_cols\n",
    "    },\n",
    "    'dataset_used': str(DATA_PATH.name),\n",
    "    'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'status': metrics.get('status', {})\n",
    "}\n",
    "with open(REPORT_PATH, 'w', encoding='utf-8') as f:\n",
    "    json.dump(report, f, indent=2)\n",
    "print('Saved report ->', REPORT_PATH)\n",
    "\n",
    "# Display report preview\n",
    "print('\\nReport snapshot:')\n",
    "for k,v in report['metrics'].items():\n",
    "    print(f\" {k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d33610",
   "metadata": {},
   "source": [
    "## 7. Validation (Reload & Re-Evaluate)\n",
    "Reload the exported pipeline (`best_model.pkl`), run predictions with the embedded threshold logic, and confirm metrics match expected tuned performance (≈ Accuracy 0.8804, F1 0.8932, ROC AUC 0.9052)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "165454f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reloaded metrics: {'accuracy': 0.875, 'precision': 0.8911, 'recall': 0.8824, 'f1': 0.8867, 'roc_auc': 0.9063}\n",
      "\n",
      "Consistency check (reloaded vs report):\n",
      " accuracy: reload=0.8750 report=0.8750 Δ=+0.0000 [MATCH]\n",
      " precision: reload=0.8911 report=0.8911 Δ=+0.0000 [MATCH]\n",
      " recall: reload=0.8824 report=0.8824 Δ=+0.0000 [MATCH]\n",
      " f1: reload=0.8867 report=0.8867 Δ=+0.0000 [MATCH]\n",
      " roc_auc: reload=0.9063 report=0.9063 Δ=+0.0000 [MATCH]\n",
      "==============================\n",
      "✅ Final Model Exported\n",
      "==============================\n",
      "Model: Threshold-Tuned SVC\n",
      "Accuracy: 0.8750\n",
      "F1 Score: 0.8867\n",
      "ROC AUC: 0.9063\n",
      "Decision Threshold: 0.617\n",
      "Params source: best_model_report.json\n",
      "Saved at: ..\\models\\best_model.pkl\n",
      "Report at: ..\\results\\best_model_report.json\n",
      "==============================\n",
      "==============================\n"
     ]
    }
   ],
   "source": [
    "# Reload and validate\n",
    "reloaded = joblib.load(MODEL_EXPORT_PATH)\n",
    "re_y_pred = reloaded.predict(X_test)\n",
    "re_proba = reloaded.predict_proba(X_test)[:,1]\n",
    "re_metrics = {\n",
    "    'accuracy': round(accuracy_score(y_test, re_y_pred), 4),\n",
    "    'precision': round(precision_score(y_test, re_y_pred, zero_division=0), 4),\n",
    "    'recall': round(recall_score(y_test, re_y_pred, zero_division=0), 4),\n",
    "    'f1': round(f1_score(y_test, re_y_pred, zero_division=0), 4),\n",
    "    'roc_auc': round(roc_auc_score(y_test, re_proba), 4)\n",
    "}\n",
    "print('Reloaded metrics:', re_metrics)\n",
    "\n",
    "# Load report to cross-check\n",
    "with open(REPORT_PATH, 'r', encoding='utf-8') as f:\n",
    "    saved_report = json.load(f)\n",
    "rep_metrics = saved_report.get('metrics', {})\n",
    "\n",
    "# Consistency check\n",
    "print('\\nConsistency check (reloaded vs report):')\n",
    "for k in ['accuracy','precision','recall','f1','roc_auc']:\n",
    "    rv = re_metrics[k]; sv = rep_metrics.get(k)\n",
    "    delta = rv - sv if sv is not None else float('nan')\n",
    "    status = 'MATCH' if sv is not None and abs(delta) <= 0.002 else 'MISMATCH'\n",
    "    print(f\" {k}: reload={rv:.4f} report={sv:.4f} Δ={delta:+.4f} [{status}]\")\n",
    "\n",
    "print('='*30)\n",
    "print('✅ Final Model Exported')\n",
    "print('='*30)\n",
    "print('Model: Threshold-Tuned SVC')\n",
    "print(f\"Accuracy: {re_metrics['accuracy']:.4f}\")\n",
    "print(f\"F1 Score: {re_metrics['f1']:.4f}\")\n",
    "print(f\"ROC AUC: {re_metrics['roc_auc']:.4f}\")\n",
    "print(f\"Decision Threshold: {SELECTED_THRESHOLD}\")\n",
    "print('Params source: best_model_report.json' if (RESULTS_DIR / 'best_model_report.json').exists() else 'Params source: defaults (report missing)')\n",
    "print(f'Saved at: {MODEL_EXPORT_PATH}')\n",
    "print(f'Report at: {REPORT_PATH}')\n",
    "print('='*30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
