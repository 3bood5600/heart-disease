{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66f863a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------\n",
    "# Core Imports & Configuration\n",
    "# Purpose: Centralize ALL library imports & global styles \n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Standard Library\n",
    "import warnings\n",
    "import time\n",
    "from collections import namedtuple\n",
    "\n",
    "# Data Handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "\n",
    "# Statistics / Distributions\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Machine Learning & Preprocessing\n",
    "from sklearn.preprocessing import (\n",
    "    StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures, LabelEncoder, OneHotEncoder\n",
    ")\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold\n",
    ")\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,\n",
    "    classification_report, adjusted_rand_score, silhouette_score\n",
    ")\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import (\n",
    "    RandomForestClassifier, RandomForestRegressor, VotingClassifier, StackingClassifier\n",
    ")\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE, SelectKBest, chi2, f_classif\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Imbalanced Learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTEENN, SMOTETomek\n",
    "from imblearn.pipeline import Pipeline  # unified Pipeline (works for normal + sampling steps)\n",
    "\n",
    "# Gradient Boosting (XGBoost)\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Style & Warning Control\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8')\n",
    "\n",
    "print('All imports consolidated. Environment ready.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12bdeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# DATA LOADING & INITIAL AUDIT\n",
    "# ===============================\n",
    "# 1. Read raw CSV into DataFrame and create a working copy.\n",
    "df = pd.read_csv(\"../data/heart_disease_uci.csv\")\n",
    "df_model = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025c088f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# PREPROCESSING STEP: COLUMN PRUNING\n",
    "# ===============================\n",
    "# Remove identifier / source columns that carry no predictive signal.\n",
    "# (Prevents data leakage and reduces noise.)\n",
    "df_model.drop(columns=['id','dataset'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e996c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FEATURE TYPE IDENTIFICATION\n",
    "# ===============================\n",
    "# Separate categorical (object) vs numeric (int/float) to drive tailored imputation,\n",
    "# encoding, and scaling decisions in subsequent steps.\n",
    "categorical_cols = df_model.select_dtypes(include=['object']).columns\n",
    "numerical_cols = df_model.select_dtypes(include=['int64', 'float64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ce348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FEATURE ENGINEERING\n",
    "# ===============================\n",
    "# Create domain‑inspired derived metrics intended to capture composite risk signals.\n",
    "# chol_per_age: normalizes cholesterol by age to adjust for natural aging effects.\n",
    "# heart_rate_reserve: proxy for cardiovascular capacity (max vs resting pressure influence).\n",
    "# risk_score: simple additive heuristic combining known binary risk factors.\n",
    "df_model['chol_per_age'] = df_model['chol'] / (df_model['age'] + 1e-5)\n",
    "df_model['heart_rate_reserve'] = df_model['thalch'] - df_model['trestbps']\n",
    "df_model['risk_score'] = (\n",
    "    (df_model['age'] > 50).astype(int) +\n",
    "    (df_model['chol'] > 240).astype(int) +\n",
    "    (df_model['fbs'] == 1).astype(int)\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e8b8d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Structural double‑check after engineered feature additions / imputations.\n",
    "df_model.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5eac1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ADVANCED IMPUTATION PIPELINE\n",
    "# ===============================\n",
    "# Strategy Overview:\n",
    "# 1. Median impute robust numeric columns individually.\n",
    "# 2. Mode impute low-missing categorical columns.\n",
    "# 3. KNN-impute 'ca' leveraging correlated numeric context.\n",
    "# 4. Model-based (RandomForest) imputation for high-missing categorical targets: slope & thal.\n",
    "# 5. Preserve schema & restore into main working frame.\n",
    "\n",
    "# 1. Copy original\n",
    "df_temp = df_model.copy()\n",
    "\n",
    "# 2. Median imputation for specified numeric columns (robust to outliers compared to mean)\n",
    "numeric_median_cols = ['chol_per_age','heart_rate_reserve','trestbps','chol','thalch','oldpeak']\n",
    "for col in numeric_median_cols:\n",
    "    if col in df_temp.columns:\n",
    "        df_temp[col] = df_temp[col].astype(float)\n",
    "        if df_temp[col].isna().any():\n",
    "            df_temp[col].fillna(df_temp[col].median(), inplace=True)\n",
    "\n",
    "# 3. Mode imputation for specified categorical columns (preserves most frequent valid category)\n",
    "categorical_mode_cols = ['restecg','fbs','exang']\n",
    "for col in categorical_mode_cols:\n",
    "    if col in df_temp.columns and df_temp[col].isna().any():\n",
    "        df_temp[col].fillna(df_temp[col].mode(dropna=True)[0], inplace=True)\n",
    "\n",
    "# 4. KNN imputation for 'ca' (captures multivariate structure vs single-column fill)\n",
    "if 'ca' in df_temp.columns:\n",
    "    df_temp['ca'] = df_temp['ca'].replace(['?', 'NA', 'N/A', 'na', ''], np.nan)\n",
    "    knn_features = [c for c in numeric_median_cols if c in df_temp.columns]\n",
    "    if knn_features:\n",
    "        knn_df = df_temp[['ca'] + knn_features].copy()\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        imputed = imputer.fit_transform(knn_df)\n",
    "        df_temp['ca'] = imputed[:, 0]\n",
    "    else:\n",
    "        imputer = KNNImputer(n_neighbors=5)\n",
    "        df_temp[['ca']] = imputer.fit_transform(df_temp[['ca']])\n",
    "    df_temp['ca'] = df_temp['ca'].round().astype(int)\n",
    "\n",
    "# 5. Model-based imputation for high-missing categorical: slope & thal\n",
    "high_missing_targets = ['slope','thal']\n",
    "\n",
    "def detect_categorical(df):\n",
    "    \"\"\"Heuristic selection: include object columns and low-cardinality integers.\"\"\"\n",
    "    cats = []\n",
    "    for c in df.columns:\n",
    "        if c in ['num'] + high_missing_targets:\n",
    "            continue\n",
    "        if df[c].dtype == 'object':\n",
    "            cats.append(c)\n",
    "        elif pd.api.types.is_integer_dtype(df[c]) and df[c].nunique() <= 10:\n",
    "            cats.append(c)\n",
    "    return sorted(set(cats))\n",
    "\n",
    "for target_col in high_missing_targets:\n",
    "    if target_col not in df_temp.columns:\n",
    "        continue\n",
    "    missing_mask = df_temp[target_col].isna()\n",
    "    n_missing = missing_mask.sum()\n",
    "    if n_missing == 0:\n",
    "        continue\n",
    "    print(f\"Imputing {n_missing} missing values in '{target_col}' via model-based approach...\")\n",
    "    feature_cols = [c for c in df_temp.columns if c not in ['num'] + high_missing_targets]\n",
    "    X_full = df_temp[feature_cols].copy()\n",
    "    y_full = df_temp[target_col]\n",
    "    categorical_feats = detect_categorical(df_temp[feature_cols])\n",
    "    numeric_feats = [c for c in feature_cols if c not in categorical_feats]\n",
    "    X_train = X_full[~missing_mask]\n",
    "    y_train = y_full[~missing_mask]\n",
    "    X_pred = X_full[missing_mask]\n",
    "    preprocessor = ColumnTransformer([\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_feats),\n",
    "        ('num', 'passthrough', numeric_feats)\n",
    "    ])\n",
    "    if y_train.nunique() <= 15 and y_train.dtype != float:\n",
    "        model = RandomForestClassifier(n_estimators=300, random_state=42)\n",
    "    else:\n",
    "        model = RandomForestRegressor(n_estimators=300, random_state=42)\n",
    "    pipe = Pipeline([('prep', preprocessor), ('model', model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_pred)\n",
    "    df_temp.loc[missing_mask, target_col] = y_pred\n",
    "    if pd.api.types.is_integer_dtype(y_train) or (y_train.dtype == 'O') or (y_train.nunique() <= 15):\n",
    "        try:\n",
    "            df_temp[target_col] = df_temp[target_col].round().astype(int)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "# 6. Finalize imputed frame\n",
    "df_model = df_temp.copy()\n",
    "print(\"Remaining missing values after preprocessing:\")\n",
    "print(df_model.isna().sum()[df_model.isna().sum() > 0])\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ff82bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# TARGET BINARIZATION\n",
    "# ===============================\n",
    "# Convert multi-class severity target 'num' into binary indicator: 1 = any disease, 0 = none.\n",
    "df_model['num'] = df_model['num'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5d7f469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# ONE-HOT ENCODING\n",
    "# ===============================\n",
    "# Expand categorical features into dummy variables (drop_first to avoid linear dependence).\n",
    "df_model = pd.get_dummies(df_model, columns=categorical_cols, drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ec5190c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5046331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================\n",
    "# FEATURE / TARGET SPLIT + SCALING\n",
    "# ===============================\n",
    "# Separate predictors (X) and binary target (y), then apply StandardScaler to normalize variance.\n",
    "X = df_model.drop('num', axis=1)\n",
    "y = df_model['num']\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "X_clean = pd.DataFrame(X_scaled, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992b5dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model.to_csv(\"../data/cleaned_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
